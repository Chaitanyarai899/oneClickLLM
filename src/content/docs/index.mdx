---
title: Deploy LLM Inference APIs within seconds on Akash Network
description: Deploy any open source Large Language models from Huggingface to Akash network with zero configuration
template: splash
hero:
  tagline: Deploy any open source Large Language model (LLM) from Huggingface or Ollama Registry to Akash network with zero configuration
  image:
    file: ../../assets/ai.svg
  actions:
    - text: Deploy your own LLM
      link: /deploy-you-own-llm
      icon: right-arrow
      variant: primary
    - text: Free Demo API
      link: /Free demo API/example/
      icon: external
---

import { Card, CardGrid } from "@astrojs/starlight/components";

### Streamling LLM Infrence API Deployment on Akash Network via Ollama, vLLM and Llama-cpp

Deploy open source models within seconds on Akash Network:

1. Select the Model of your choice from Huggingface or Ollama registry.
2. Hit Deploy to get a properly configured SDL file with infernce api of the selected model.
3. Copy the sdl and use it to deploy on cloudmos or console!

- The deployment process is fully automated and managed by the Akash network, which ensures that the service is highly available, scalable, and secure.

-------------------------------------------------------------
<CardGrid stagger>

  <Card  title="Free Demo API" icon="open-book">
  [Click here](guides/example/)  to use our free API
  </Card>
  <Card  title="Endpoint Documentation" icon="open-book">
  [Click here](guides/example/) to see the user docs
  </Card>
</CardGrid>
